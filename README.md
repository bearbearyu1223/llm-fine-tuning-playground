# Fine-tuning LLMs using PEFT-LoRA 

## Introduction
In today's business landscape, we are surrounded by a wealth of opportunities to utilize advanced technology. Think of large language models(LLMs) as versatile tools in our toolkit: we can customize them for a variety of specific downstream tasks, a process known as "fine-tuning". However, a challenge arises in that each fine-tuned model typically maintains the same parameter size as the original. Therefore, managing multiple fine-tuned models requires careful consideration of factors like model accuracy performance, memory management, power consumption, inference latency, and disk utilization.

Parameter-Efficient Fine-Tuning (PEFT) methods offer a streamlined way to adapt pre-trained LLMs, often referred to as the *base models*, for a variety of specific downstream tasks. These tasks can include applications such like text summarization, question answering, image generation. Traditional full fine-tuning of all parameters in a base model demands substantial computational resources. In contrast, PEFT approaches concentrate on optimizing a significantly smaller subset of parameters, commonly known as *adapters*. These adapters are designed to work in tandem with the unchanged base model, allowing for tailored product experience for each specific use case. Moreover, recent advancements in state-of-the-art (SoTA) PEFT methods have shown that they can achieve performance levels comparable to those of fully fine-tuned base models, but with a fraction of the computational and storage overhead. 

I've shared a sample Colab notebook that guides through a straightforward PEFT process via LoRA ([Low-Rank Adaptation](https://browse.arxiv.org/pdf/2106.09685.pdf)), utilizing the [libraries developed by HuggingFace](https://github.com/huggingface/peft), to fine-tune [Falcon-7b-sharded model](https://huggingface.co/vilsonrodrigues/falcon-7b-sharded) on [samsum dataset](https://huggingface.co/datasets/samsum), for summarization task of "message-like conversations". This illustrates how PEFT can optimize the fine-tuning experiments of LLMs in a resource-efficient manner. 

In addition, I utilized `GPT-3.5-turbo` to evaluate the quality of generated summaries based on input prompts, human-authored summaries, and those generated by the fine-tuned model. This approach stands in contrast to metrics like ROUGE or BERTScore, which typically depend on reference summaries for evaluationâ€”a method often deemed inadequate for abstractive summarization tasks. This methodology showcases how LLMs can contribute to both automated and human evaluations in a scalable, reliable, and efficient manner.

## Fine-Tuning via LoRA
todo
## Model Inference via the Fined-Tuned Model 
todo
## Evaluation of Summarization Quality 

In the real world, it's important to draft clear and consistent annotation guidelines to instruct human annotators on how to assess the quality of summarization generated from this fine-tuned LLM. Below is a list of metrics we will consider:

### Metric 1: Relevance
**Capturing the Essence:** The LLM will assist annotators in evaluating the relevance of a summary. Annotators will evaluate the relevance of a summary on a scale of 1 to 5, considering whether the summary effectively extracts important content from the source conversation, avoiding redundancies and excess information. With clear criteria and steps, annotators can confidently assign scores that reflect the summary's ability to convey essential details.

### Metric 2: Coherence
**Creating Clarity:** The LLM will assist annotators in evaluating the coherence of a summary. Annotators will rate summaries from 1 to 5, focusing on the summary's organization and logical flow. Clear guidelines enable annotators to determine how well the summary presents information in a structured and coherent manner.

### Metric 3: Consistency
**Factually Sound:** The LLM will assist annotators in evaluating the consistency of a summary. Annotators will assess summaries for factual alignment with the source conversation, rating them from 1 to 5. SummarizeMaster ensures that annotators identify and penalize summaries containing factual inaccuracies or hallucinated facts, enhancing the reliability of the evaluation process.

### Metric 4: Fluency
**Language Excellence:** The LLM will assist annotators in evaluating the fluency of a summary. Fluency is a critical aspect of summary evaluation. Annotators will assess summaries for grammar, spelling, punctuation, word choice, and sentence structure, assigning scores from 1 to 5.

The above instructions are formulated as prompt template to facilitate consistent and data-driven assessment of human-generated summaries and those produced by the fine-tuned LLM. This approach enforces consistency, standardization, and efficiency in an otherwise labor-intensive manual evaluation process.
