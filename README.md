# Fine-tuning LLMs using PEFT-LoRA 

## Introduction
In today's business landscape, we are surrounded by a wealth of opportunities to utilize advanced technology. Think of large language models(LLMs) as versatile tools in our toolkit: we can customize them for a variety of specific downstream tasks, a process known as "fine-tuning". However, a challenge arises in that each fine-tuned model typically maintains the same parameter size as the original. Therefore, managing multiple fine-tuned models requires careful consideration of factors like model accuracy performance, memory management, power consumption, inference latency, and disk utilization.

Parameter-Efficient Fine-Tuning (PEFT) methods offer a streamlined way to adapt pre-trained LLMs, often referred to as the *base models*, for a variety of specific downstream tasks. These tasks can include applications such like text summarization, question answering, image generation. Traditional full fine-tuning of all parameters in a base model demands substantial computational resources. In contrast, PEFT approaches concentrate on optimizing a significantly smaller subset of parameters, commonly known as *adapters*. These adapters are designed to work in tandem with the unchanged base model, allowing for tailored product experience for each specific use case. Moreover, recent advancements in state-of-the-art (SoTA) PEFT methods have shown that they can achieve performance levels comparable to those of fully fine-tuned base models, but with a fraction of the computational and storage overhead. 

I've shared a sample Colab notebook that guides through a straightforward PEFT process via LoRA ([Low-Rank Adaptation](https://browse.arxiv.org/pdf/2106.09685.pdf)), utilizing the [libraries developed by HuggingFace](https://github.com/huggingface/peft), to fine-tune [Falcon-7b-sharded model](https://huggingface.co/vilsonrodrigues/falcon-7b-sharded) on [samsum dataset](https://huggingface.co/datasets/samsum), for summarization task of "message-like conversations". This illustrates how PEFT can optimize the fine-tuning experiments of LLMs in a resource-efficient manner. 

In addition, I utilized GPT-4 to evaluate the quality of generated summaries based on input prompts, human-authored summaries, and those generated by the fine-tuned model. This approach stands in contrast to metrics like ROUGE or BERTScore, which typically depend on reference summaries for evaluationâ€”a method often deemed inadequate for abstractive summarization tasks. This methodology showcases how LLMs can contribute to both automated and human evaluations in a scalable, reliable, and efficient manner.




