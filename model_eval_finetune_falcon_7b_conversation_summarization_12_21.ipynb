{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bearbearyu1223/llm-fine-tuning-playground/blob/main/model_eval_finetune_falcon_7b_conversation_summarization_12_21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for evaluating a fine-tuned [Falcon-7b-sharded model](https://huggingface.co/vilsonrodrigues/falcon-7b-sharded) on [samsum](https://huggingface.co/datasets/samsum) dataset. Model card can be found [here](https://huggingface.co/bearbearyu1223/falcon_7b_LoRA_r16_dialogue_summarization_12_13_2023).\n"
      ],
      "metadata": {
        "id": "MuWvHMGIOoWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installs and Imports Frameworks"
      ],
      "metadata": {
        "id": "LOWAQZuH6hHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub==0.19.4\n",
        "!pip install -q -U accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install transformers==4.36.0\n",
        "!pip install datasets==2.15.0 Tokenizers==0.15.0\n",
        "!pip install -q bitsandbytes\n",
        "!pip install openai\n",
        "!pip install --user -U nltk\n",
        "!pip install py7zr\n",
        "!pip install evaluate\n",
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "-bys4q4u45v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ag4d-fyImNZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the dataset from hugging face and Formatting the Training Dataset\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "83DM_0Jh6UGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"samsum\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']\n",
        "dataset"
      ],
      "metadata": {
        "id": "YavG6n_v53kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "YjDjJTJA8-nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading PEFT model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "PEFT_MODEL = \"bearbearyu1223/falcon_7b_LoRA_r16_dialogue_summarization_12_20_2023\"\n",
        "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
        "peft_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    return_dict=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)"
      ],
      "metadata": {
        "id": "O75HchDaIbxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "if peft_tokenizer.pad_token_id is None:\n",
        "   peft_tokenizer.pad_token = peft_tokenizer.eos_token\n",
        "   peft_model.config.pad_token_id = peft_tokenizer.eos_token_id\n",
        "   peft_model.config.pad_token = peft_tokenizer.eos_token\n",
        "   peft_base_model.config.pad_token_id = peft_tokenizer.eos_token_id\n",
        "   peft_base_model.config.pad_token = peft_tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "efuMp1MB8C7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_incomplete_sentences(text):\n",
        "\n",
        "  # Compile the regular expression for complete sentences.\n",
        "  complete_sentence_regex = re.compile(r\"(^.*[\\.\\?!]|^\\S[^.\\?!]*)\")\n",
        "\n",
        "  # Find all of the complete sentences in the text.\n",
        "  complete_sentences = complete_sentence_regex.findall(text)\n",
        "  text = \" \".join(complete_sentences)\n",
        "\n",
        "  # Return the text with the incomplete sentences removed.\n",
        "  return text\n",
        "\n",
        "# Generate Summarization\n",
        "def get_summary(dialogue, min_length=10, max_length=512, verbose=False):\n",
        "  prompt= \"### Instruction:\\n{instruction}\\n\\n### Dialogue:\\n{dialogue}\\n\\n### Summary:\\n\".format(instruction=\"Summarize the Dialogue below.\", dialogue=dialogue)\n",
        "  if verbose:\n",
        "    print(prompt)\n",
        "\n",
        "  peft_encoding = peft_tokenizer(prompt, truncation=True, return_tensors=\"pt\").to(torch.device(\"cuda:0\"))\n",
        "  peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(do_sample=True,\n",
        "                                                                                                         num_beams=2,\n",
        "                                                                                                         no_repeat_ngram_size=3,\n",
        "                                                                                                         max_length=max_length,\n",
        "                                                                                                         min_length=min_length,\n",
        "                                                                                                         pad_token_id = peft_tokenizer.eos_token_id,\n",
        "                                                                                                         eos_token_id = peft_tokenizer.eos_token_id,\n",
        "                                                                                                         attention_mask = peft_encoding.attention_mask,\n",
        "                                                                                                         temperature=0.1, top_p=0.9, repetition_penalty=30.0, num_return_sequences=1,))\n",
        "  peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  sub = \"### Summary:\"\n",
        "  raw_summary = peft_text_output.split(sub)[1]\n",
        "  post_processed_summary = remove_incomplete_sentences(raw_summary.strip())\n",
        "\n",
        "  return post_processed_summary"
      ],
      "metadata": {
        "id": "zyHt2-Bh8_qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inferences"
      ],
      "metadata": {
        "id": "Zt2-x5gV2yae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "tqdm.pandas()\n",
        "\n",
        "test_index=11\n",
        "dialogue=test_dataset[test_index]['dialogue']\n",
        "summary=test_dataset[test_index]['summary']\n",
        "peft_output=get_summary(dialogue,verbose=True)\n",
        "\n",
        "print(\"Human Summary:\")\n",
        "print(summary)\n",
        "print(\"PEFT Summary:\")\n",
        "print(peft_output)"
      ],
      "metadata": {
        "id": "HJY6sq1U3GlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = []\n",
        "references = []\n",
        "predictions = []\n",
        "\n",
        "for sample in tqdm(test_dataset):\n",
        "  summary=sample['summary']\n",
        "  dialogue=sample['dialogue']\n",
        "  peft_summary=get_summary(dialogue=dialogue)\n",
        "  inputs.append(dialogue)\n",
        "  references.append(summary)\n",
        "  predictions.append(peft_summary)"
      ],
      "metadata": {
        "id": "Abd3JxTQzqv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dict = {'inputs':inputs, 'summary_human_baseline':references, 'summary_peft_baseline':predictions}\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('falcon_7b_LoRA_r16_dialogue_summarization_12_21_2023_results.csv')"
      ],
      "metadata": {
        "id": "Clso9uvRELpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "tJ2ZDVZx2ilQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation via Rogue Score"
      ],
      "metadata": {
        "id": "br3hkpg3TgK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load('rouge')\n",
        "results = metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(f\"Rogue1: {results['rouge1']* 100:2f}%\")\n",
        "print(f\"rouge2: {results['rouge2']* 100:2f}%\")\n",
        "print(f\"rougeL: {results['rougeL']* 100:2f}%\")\n",
        "print(f\"rougeLsum: {results['rougeLsum']* 100:2f}%\")"
      ],
      "metadata": {
        "id": "dvKZQ693TaHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation via LLMs"
      ],
      "metadata": {
        "id": "on6nHz6dTqDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "Re5HzhRi1age"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation prompt template\n",
        "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You will be given one summary written for an conversation. Your task is to rate the summary on one metric.\n",
        "Please make sure you read and understand these instructions very carefully.\n",
        "\n",
        "Evaluation Criteria:\n",
        "\n",
        "{criteria}\n",
        "\n",
        "Evaluation Steps:\n",
        "\n",
        "{steps}\n",
        "\n",
        "Example:\n",
        "\n",
        "Source Text:\n",
        "\n",
        "{conversation}\n",
        "\n",
        "Summary:\n",
        "\n",
        "{summary}\n",
        "\n",
        "Evaluation Form (scores ONLY):\n",
        "\n",
        "- {metric_name}\n",
        "\"\"\"\n",
        "\n",
        "# Metric 1: Relevance\n",
        "\n",
        "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
        "Relevance(1-5) - selection of important content from the source. \\\n",
        "The summary should include only important information from the source conversation. \\\n",
        "Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
        "1: Poor.\n",
        "  - The summary includes very little relevant content from the source conversation. It fails to capture essential details and main points. It contains a significant amount of irrelevant or redundant information.\n",
        "\n",
        "2: Limited.\n",
        "  - The summary includes some relevant content but overlooks key details and main points from the source. There may be redundancies or minor irrelevant information present. The selection of important content is inadequate.\n",
        "\n",
        "3: Moderate.\n",
        "  - The summary includes a moderate amount of relevant content from the source conversation. It captures the main points but may lack depth in some areas. There might be occasional redundancies or minor irrelevant information.\n",
        "\n",
        "4: Good.\n",
        "  - The summary includes a good amount of relevant content from the source conversation. It effectively captures and conveys the main points and key details. Redundancies and irrelevant information are minimal or non-disruptive.\n",
        "\n",
        "5: Excellent.\n",
        "  - The summary demonstrates excellent relevance by selecting and including all important content from the source conversation. It provides a clear and concise representation of the main points and key details. There are no redundancies or excess information; every sentence contributes to the overall understanding.\n",
        "\"\"\"\n",
        "\n",
        "RELEVANCY_SCORE_STEPS = \"\"\"\n",
        "1. Read the summary and the source conversation carefully.\n",
        "2. Compare the summary to the source conversation and identify the main points of the conversation.\n",
        "3. Assess how well the summary covers the main points of the conversation, and how much irrelevant or redundant information it contains.\n",
        "4. Assign a relevance score from 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
        "\"\"\"\n",
        "\n",
        "# Metric 2: Coherence\n",
        "\n",
        "COHERENCE_SCORE_CRITERIA = \"\"\"\n",
        "Coherence(1-5) - the collective quality of all sentences. \\\n",
        "The summary should be well-structured and well-organized. \\\n",
        "The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic. \\\n",
        "Annotators were also instructed to penalize summaries that were not logically well-organized based on the information from the conversation\n",
        "\n",
        "1: Poor.\n",
        "  - The summary is highly disorganized and lacks any logical flow. Sentences are disconnected, making it difficult to follow the conversation. Information is scattered, and the summary fails to build a coherent narrative.\n",
        "\n",
        "2: Limited.\n",
        "  - The summary has some organization, but it is still challenging to follow. There is a loose attempt to group related sentences, but transitions are weak. The summary does not effectively build a coherent body of information.\n",
        "\n",
        "3: Moderate.\n",
        "  - The summary demonstrates a moderate level of organization. Sentences are somewhat connected, making it somewhat easier to follow. There is an attempt to build a coherent narrative, but it could be improved.\n",
        "\n",
        "4: Good.\n",
        "  - The summary is well-structured and organized. Sentences are logically connected, leading to a clear and coherent narrative. Information is presented in a way that progressively builds upon the previous sentences.\n",
        "\n",
        "5: Excellent.\n",
        "  - The summary is highly organized and exceptionally coherent. Sentences flow seamlessly, creating a smooth and logical progression of information. It effectively builds a cohesive and complete narrative that is easy to follow and understand.\n",
        "\"\"\"\n",
        "\n",
        "COHERENCE_SCORE_STEPS = \"\"\"\n",
        "1. Read the conversation carefully and identify the main topic and key points.\n",
        "2. Read the summary and compare it to the conversation. Check if the summary covers the main topic and key points of the conversation,\n",
        "and if it presents them in a clear and logical order.\n",
        "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
        "\"\"\"\n",
        "\n",
        "# Metric 3: Consistency\n",
        "\n",
        "CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
        "Consistency(1-5) - the factual alignment between the summary and the summarized source. \\\n",
        "A factually consistent summary contains only statements that are entailed by the source conversation. \\\n",
        "Annotators were also asked to penalize summaries that contained made-up facts or unsupported details from the original discussion.\n",
        "1: Poor.\n",
        "  - The summary contains numerous factual errors that do not match the source conversation. It includes fabricated or entirely false information. There is a substantial disparity between the summary and the original conversation.\n",
        "\n",
        "2: Limited.\n",
        "  - The summary has multiple factual errors or inconsistencies with the source conversation. It may incorporate some made-up facts or unsupported details from the original discussion. The alignment between the summary and source is weak.\n",
        "\n",
        "3. Moderate.\n",
        "  - The summary shows moderate consistency with the source conversation. While it generally aligns with the original discussion, occasional factual errors or minor discrepancies may appear. Some statements in the summary may lack full support from the source.\n",
        "\n",
        "4. Good.\n",
        "  - The summary is factually consistent with the source conversation. It accurately reflects the main points and details of the original discussion. Factual inaccuracies or hallucinated facts are minimal or non-existent.\n",
        "\n",
        "5. Excellent.\n",
        "  - The summary exhibits excellent consistency with the source conversation. It precisely and faithfully represents the factual content and details of the original discussion. There are no factual inaccuracies or hallucinated facts, and the summary aligns perfectly with the source.\n",
        "\"\"\"\n",
        "\n",
        "CONSISTENCY_SCORE_STEPS = \"\"\"\n",
        "1. Read the conversation carefully and identify the main facts and details it presents.\n",
        "2. Read the summary and compare it to the conversation. Check if the summary contains any factual errors that are not supported by the conversation.\n",
        "3. Assign a score for consistency based on the Evaluation Criteria, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
        "\"\"\"\n",
        "\n",
        "# Metric 4: Fluency\n",
        "\n",
        "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
        "Fluency(1-5): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure. \\\n",
        "Annotators were also asked to penalize summaries that contained grammar, spelling, and punctuation errors.\n",
        "1: Poor.\n",
        " - The summary contains numerous grammar, spelling, or punctuation errors. It may lack completeness, making it hard to understand. Word choice and sentence structure severely affect coherence.\n",
        "\n",
        "2: Limited.\n",
        " - The summary has several grammar, spelling, or punctuation issues. These issues significantly impact comprehension. Word choice and sentence structure need improvement.\n",
        "\n",
        "3: Moderate.\n",
        " - The summary shows moderate fluency with some noticeable language problems. Grammar, spelling, or punctuation errors don't severely hinder understanding. Word choice and sentence structure are generally acceptable but may require refinement.\n",
        "\n",
        "4: Good.\n",
        " - The summary is fluently written with minimal language errors. Language issues have minimal impact on comprehension. Word choice and sentence structure enhance clarity.\n",
        "\n",
        "5: Excellent.\n",
        " - The summary demonstrates excellent fluency in grammar, spelling, punctuation, word choice, and sentence structure. It is impeccably written with no language-related issues. Word choice and sentence structure greatly enhance overall quality and readability.\n",
        "\"\"\"\n",
        "\n",
        "FLUENCY_SCORE_STEPS = \"\"\"\n",
        "Read the summary and evaluate its fluency. Assign a fluency score from 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GGq18fHn1dPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "Y3_e-hgk1kzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_geval_score(\n",
        "    criteria: str, steps: str, conversation: str, summary: str, metric_name: str, verbose=False\n",
        "):\n",
        "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
        "        criteria=criteria,\n",
        "        steps=steps,\n",
        "        metric_name=metric_name,\n",
        "        conversation=conversation,\n",
        "        summary=summary,\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "        max_tokens=5,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    result = response.choices[0].message.content\n",
        "    if result and re.findall(\"[0-9]+\",result):\n",
        "      score_num = int(re.findall(\"[0-9]+\",result)[0])\n",
        "    else:\n",
        "      score_num = pd.NA\n",
        "\n",
        "    if verbose:\n",
        "        print('\\n{0}:{1}'.format(metric_name, score_num))\n",
        "    return score_num"
      ],
      "metadata": {
        "id": "21o-6--f2nx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metrics = {\n",
        "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
        "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
        "    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n",
        "    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
        "}"
      ],
      "metadata": {
        "id": "frD_f9TL2uRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for metric, (criteria, steps) in evaluation_metrics.items():\n",
        "    col_name_human = metric+'_human_summary'\n",
        "    dash_line = '-'.join('' for x in range(100))\n",
        "    print(dash_line)\n",
        "    print(\"Generating human baseline summarization score for {0}\".format(metric))\n",
        "    df[col_name_human] = df.progress_apply(lambda x: get_geval_score(criteria=criteria, steps=steps, conversation=x.inputs,\n",
        "                                                            summary=x.summary_human_baseline,\n",
        "                                                            metric_name=metric), axis=1)\n",
        "\n",
        "    print(\"Human Summary Score for {0} : {1}\".format(metric, df[col_name_human].describe()))\n",
        "\n",
        "    print(dash_line)\n",
        "    col_name_model = metric+'_peft_model_summary'\n",
        "    print(\"Generating PEFT baseline summarization score for {0}\".format(metric))\n",
        "    df[col_name_model] = df.progress_apply(lambda x: get_geval_score(criteria=criteria, steps=steps, conversation=x.inputs,\n",
        "                                                            summary=x.summary_peft_baseline,\n",
        "                                                            metric_name=metric), axis=1)\n",
        "    print(\"PEFT Model Summary Score for {0} : {1}\".format(metric, df[col_name_model].describe()))\n",
        "\n",
        "df.to_csv('falcon_7b_LoRA_r16_dialogue_summarization_12_21_2023_results_eval.csv')"
      ],
      "metadata": {
        "id": "nZM87Ov92wt7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}