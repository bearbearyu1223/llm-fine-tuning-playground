{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bearbearyu1223/llm-fine-tuning-playground/blob/main/model_eval_finetune_falcon_7b_conversation_summarization_12_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for evaluating a fine-tuned [Falcon-7b-sharded model](https://huggingface.co/vilsonrodrigues/falcon-7b-sharded) on [samsum](https://huggingface.co/datasets/samsum) dataset. Model card can be found [here](https://huggingface.co/bearbearyu1223/falcon_7b_LoRA_r16_dialogue_summarization_12_13_2023).\n"
      ],
      "metadata": {
        "id": "MuWvHMGIOoWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installs and Imports Frameworks"
      ],
      "metadata": {
        "id": "LOWAQZuH6hHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub==0.19.4\n",
        "!pip install -q -U accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install transformers==4.36.0\n",
        "!pip install datasets==2.15.0 Tokenizers==0.15.0\n",
        "!pip install -q bitsandbytes\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "-bys4q4u45v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ag4d-fyImNZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the dataset from hugging face and Formatting the Training Dataset\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "83DM_0Jh6UGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py7zr"
      ],
      "metadata": {
        "id": "GlJ99IclkwFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"samsum\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']\n",
        "dataset"
      ],
      "metadata": {
        "id": "YavG6n_v53kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "YjDjJTJA8-nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading PEFT model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "PEFT_MODEL = \"bearbearyu1223/falcon_7b_LoRA_r16_dialogue_summarization_12_13_2023\"\n",
        "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
        "peft_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    return_dict=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n",
        "\n",
        "peft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "peft_tokenizer.pad_token = peft_tokenizer.eos_token"
      ],
      "metadata": {
        "id": "O75HchDaIbxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_incomplete_sentences(text):\n",
        "\n",
        "  # Compile the regular expression for complete sentences.\n",
        "  complete_sentence_regex = re.compile(r\"(^.*[\\.\\?!]|^\\S[^.\\?!]*)\")\n",
        "\n",
        "  # Find all of the complete sentences in the text.\n",
        "  complete_sentences = complete_sentence_regex.findall(text)\n",
        "  text = \" \".join(complete_sentences)\n",
        "\n",
        "  # Return the text with the incomplete sentences removed.\n",
        "  return text\n",
        "\n",
        "# Generate Summarization\n",
        "def get_summary(dialogue, max_new_tokens=10, verbose=False):\n",
        "  prompt= \"### Instruction:\\n{instruction}\\n\\n### Dialogue:\\n{dialogue}\\n\\n### Summary:\\n\".format(instruction=\"Summarize the Dialogue below.\", dialogue=dialogue)\n",
        "  if verbose:\n",
        "    print(prompt)\n",
        "\n",
        "  peft_encoding = peft_tokenizer(prompt, return_tensors=\"pt\").to(torch.device(\"cuda:0\"))\n",
        "  peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=max_new_tokens,\n",
        "                                                                                                         pad_token_id = peft_tokenizer.eos_token_id,\n",
        "                                                                                                         eos_token_id = peft_tokenizer.eos_token_id,\n",
        "                                                                                                         attention_mask = peft_encoding.attention_mask,\n",
        "                                                                                                         temperature=0.1, top_p=0.8, repetition_penalty=10.0, num_return_sequences=1,))\n",
        "  peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  sub = \"### Summary:\"\n",
        "  raw_summary = peft_text_output.split(sub)[1]\n",
        "  post_processed_summary = remove_incomplete_sentences(raw_summary.strip())\n",
        "\n",
        "  return post_processed_summary"
      ],
      "metadata": {
        "id": "zyHt2-Bh8_qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inferences"
      ],
      "metadata": {
        "id": "Zt2-x5gV2yae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "inputs = []\n",
        "references = []\n",
        "predictions = []\n",
        "condense_rate=0.5\n",
        "\n",
        "for sample in tqdm(test_dataset):\n",
        "  summary=sample['summary']\n",
        "  dialogue=sample['dialogue']\n",
        "  max_new_tokens=max(int(len(dialogue.strip().split())*condense_rate), int(len(summary.strip().split())))\n",
        "  peft_summary=get_summary(dialogue=dialogue, max_new_tokens=max_new_tokens)\n",
        "  inputs.append(dialogue)\n",
        "  references.append(summary)\n",
        "  predictions.append(peft_summary)"
      ],
      "metadata": {
        "id": "HJY6sq1U3GlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dict = {'inputs':inputs, 'summary_human_baseline':references, 'summary_peft_baseline':predictions}\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('falcon_7b_LoRA_r16_dialogue_summarization_12_13_2023_results.csv')"
      ],
      "metadata": {
        "id": "Clso9uvRELpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "tJ2ZDVZx2ilQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "Re5HzhRi1age"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation prompt template\n",
        "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You will be given one summary written for an conversation. Your task is to rate the summary on one metric.\n",
        "Please make sure you read and understand these instructions very carefully.\n",
        "\n",
        "Evaluation Criteria:\n",
        "\n",
        "{criteria}\n",
        "\n",
        "Evaluation Steps:\n",
        "\n",
        "{steps}\n",
        "\n",
        "Example:\n",
        "\n",
        "Source Text:\n",
        "\n",
        "{conversation}\n",
        "\n",
        "Summary:\n",
        "\n",
        "{summary}\n",
        "\n",
        "Evaluation Form (scores ONLY):\n",
        "\n",
        "- {metric_name}\n",
        "\"\"\"\n",
        "\n",
        "# Metric 1: Relevance\n",
        "\n",
        "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
        "Relevance(1-5) - selection of important content from the source. \\\n",
        "The summary should include only important information from the source conversation. \\\n",
        "Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
        "1: Poor.\n",
        "   - The summary includes very little relevant content from the source conversation.\n",
        "   - It fails to capture essential details and main points.\n",
        "   - Contains a significant amount of irrelevant or redundant information.\n",
        "2: Limited.\n",
        "   - The summary includes some relevant content but overlooks key details and main points from the source.\n",
        "   - There may be redundancies or minor irrelevant information present.\n",
        "   - The selection of important content is inadequate.\n",
        "3: Moderate.\n",
        "   - The summary includes a moderate amount of relevant content from the source conversation.\n",
        "   - It captures the main points but may lack depth in some areas.\n",
        "   - There might be occasional redundancies or minor irrelevant information.\n",
        "4: Good.\n",
        "   - The summary includes a good amount of relevant content from the source conversation.\n",
        "   - It effectively captures and conveys the main points and key details.\n",
        "   - Redundancies and irrelevant information are minimal or non-disruptive.\n",
        "5: Excellent.\n",
        "   - The summary demonstrates excellent relevance by selecting and including all important content from the source conversation.\n",
        "   - It provides a clear and concise representation of the main points and key details.\n",
        "   - There are no redundancies or excess information; every sentence contributes to the overall understanding.\n",
        "\"\"\"\n",
        "\n",
        "RELEVANCY_SCORE_STEPS = \"\"\"\n",
        "1. Read the summary and the source conversation carefully.\n",
        "2. Compare the summary to the source conversation and identify the main points of the conversation.\n",
        "3. Assess how well the summary covers the main points of the conversation, and how much irrelevant or redundant information it contains.\n",
        "4. Assign a relevance score from 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
        "\"\"\"\n",
        "\n",
        "# Metric 2: Coherence\n",
        "\n",
        "COHERENCE_SCORE_CRITERIA = \"\"\"\n",
        "Coherence(1-5) - the collective quality of all sentences. \\\n",
        "The summary should be well-structured and well-organized. \\\n",
        "The summary should not just be a heap of related information, but should build from sentence to a\\\n",
        "coherent body of information about a topic.\n",
        "1: Poor.\n",
        "   - The summary is highly disorganized and lacks any logical flow.\n",
        "   - Sentences are disconnected, making it difficult to follow the conversation.\n",
        "   - Information is scattered, and the summary fails to build a coherent narrative.\n",
        "2: Limited.\n",
        "   - The summary has some organization, but it is still challenging to follow.\n",
        "   - There is a loose attempt to group related sentences, but transitions are weak.\n",
        "   - The summary does not effectively build a coherent body of information.\n",
        "3: Moderate.\n",
        "   - The summary demonstrates a moderate level of organization.\n",
        "   - Sentences are somewhat connected, making it somewhat easier to follow.\n",
        "   - There is an attempt to build a coherent narrative, but it could be improved.\n",
        "4: Good.\n",
        "   - The summary is well-structured and organized.\n",
        "   - Sentences are logically connected, leading to a clear and coherent narrative.\n",
        "   - Information is presented in a way that progressively builds upon the previous sentences.\n",
        "5: Excellent.\n",
        "   - The summary is highly organized and exceptionally coherent.\n",
        "   - Sentences flow seamlessly, creating a smooth and logical progression of information.\n",
        "   - It effectively builds a cohesive and complete narrative that is easy to follow and understand.\n",
        "\"\"\"\n",
        "\n",
        "COHERENCE_SCORE_STEPS = \"\"\"\n",
        "1. Read the conversation carefully and identify the main topic and key points.\n",
        "2. Read the summary and compare it to the conversation. Check if the summary covers the main topic and key points of the conversation,\n",
        "and if it presents them in a clear and logical order.\n",
        "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
        "\"\"\"\n",
        "\n",
        "# Metric 3: Consistency\n",
        "\n",
        "CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
        "Consistency(1-5) - the factual alignment between the summary and the summarized source. \\\n",
        "A factually consistent summary contains only statements that are entailed by the source conversation. \\\n",
        "Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
        "1: Poor.\n",
        "   - The summary contains numerous factual inaccuracies that do not align with the source conversation.\n",
        "   - It includes hallucinated or completely false information.\n",
        "   - There is a significant disconnect between the summary and the original conversation.\n",
        "2: Limited.\n",
        "   - The summary contains several factual inaccuracies or inconsistencies with the source conversation.\n",
        "   - It may include some hallucinated facts or details that are not supported by the original discussion.\n",
        "   - The alignment between the summary and source is weak.\n",
        "3. Moderate.\n",
        "   - The summary demonstrates moderate consistency with the source conversation.\n",
        "   - While it generally aligns with the original discussion, there may be occasional factual inaccuracies or minor discrepancies.\n",
        "   - Some statements in the summary might not be fully supported by the source.\n",
        "4. Good.\n",
        "   - The summary is factually consistent with the source conversation.\n",
        "   - It accurately reflects the main points and details of the original discussion.\n",
        "   - Factual inaccuracies or hallucinated facts are minimal or non-existent.\n",
        "5. Excellent.\n",
        "   - The summary exhibits excellent consistency with the source conversation.\n",
        "   - It precisely and faithfully represents the factual content and details of the original discussion.\n",
        "   - There are no factual inaccuracies or hallucinated facts, and the summary aligns perfectly with the source.\n",
        "\"\"\"\n",
        "\n",
        "CONSISTENCY_SCORE_STEPS = \"\"\"\n",
        "1. Read the conversation carefully and identify the main facts and details it presents.\n",
        "2. Read the summary and compare it to the conversation. Check if the summary contains any factual errors that are not supported by the conversation.\n",
        "3. Assign a score for consistency based on the Evaluation Criteria, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
        "\"\"\"\n",
        "\n",
        "# Metric 4: Fluency\n",
        "\n",
        "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
        "Fluency(1-5): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
        "1: Poor.\n",
        "   - The summary is riddled with grammatical errors, or misspellings, or punctuation mistakes, or is not a complete sentence.\n",
        "   - It is challenging to understand due to the numerous language issues.\n",
        "   - Word choice and sentence structure are highly problematic, making the summary incoherent.\n",
        "2: Limited.\n",
        "   - The summary contains several grammatical errors, misspellings, and punctuation issues.\n",
        "   - Language problems affect comprehension to a significant degree.\n",
        "   - Word choice and sentence structure are subpar but not completely incomprehensible.\n",
        "3: Moderate.\n",
        "   - The summary exhibits moderate fluency but still contains some noticeable language issues.\n",
        "   - While there are some grammatical errors and misspellings, they do not severely hinder understanding.\n",
        "   - Word choice and sentence structure are generally acceptable but may need improvement.\n",
        "4: Good.\n",
        "   - The summary is fluently written with minimal grammatical errors, misspellings, or punctuation mistakes.\n",
        "   - Language-related issues do not significantly impact comprehension.\n",
        "   - Word choice and sentence structure are well-crafted and contribute to overall clarity.\n",
        "5: Excellent.\n",
        "   - The summary demonstrates excellent fluency in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
        "   - It is impeccably written and free of language-related issues.\n",
        "   - Word choice and sentence structure are highly polished, enhancing the overall quality and readability of the summary.\n",
        "\"\"\"\n",
        "\n",
        "FLUENCY_SCORE_STEPS = \"\"\"\n",
        "Read the summary and evaluate its fluency. Assign a fluency score from 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GGq18fHn1dPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "Y3_e-hgk1kzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_geval_score(\n",
        "    criteria: str, steps: str, conversation: str, summary: str, metric_name: str, verbose=False\n",
        "):\n",
        "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
        "        criteria=criteria,\n",
        "        steps=steps,\n",
        "        metric_name=metric_name,\n",
        "        conversation=conversation,\n",
        "        summary=summary,\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "        max_tokens=5,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    result = response.choices[0].message.content\n",
        "    if result and re.findall(\"[0-9]+\",result):\n",
        "      score_num = int(re.findall(\"[0-9]+\",result)[0])\n",
        "    else:\n",
        "      score_num = pd.NA\n",
        "\n",
        "    if verbose:\n",
        "        print('\\n{0}:{1}'.format(metric_name, score_num))\n",
        "    return score_num"
      ],
      "metadata": {
        "id": "21o-6--f2nx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metrics = {\n",
        "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
        "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
        "    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n",
        "    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
        "}"
      ],
      "metadata": {
        "id": "frD_f9TL2uRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for metric, (criteria, steps) in evaluation_metrics.items():\n",
        "    col_name_human = metric+'_human_summary'\n",
        "    dash_line = '-'.join('' for x in range(100))\n",
        "    print(dash_line)\n",
        "    print(\"Generating human baseline summarization score for {0}\".format(metric))\n",
        "    df[col_name_human] = df.progress_apply(lambda x: get_geval_score(criteria=criteria, steps=steps, conversation=x.inputs,\n",
        "                                                            summary=x.summary_human_baseline,\n",
        "                                                            metric_name=metric), axis=1)\n",
        "\n",
        "    print(\"Human Summary Score for {0} : {1}\".format(metric, df[col_name_human].describe()))\n",
        "\n",
        "    print(dash_line)\n",
        "    col_name_model = metric+'_peft_model_summary'\n",
        "    print(\"Generating PEFT baseline summarization score for {0}\".format(metric))\n",
        "    df[col_name_model] = df.progress_apply(lambda x: get_geval_score(criteria=criteria, steps=steps, conversation=x.inputs,\n",
        "                                                            summary=x.summary_peft_baseline,\n",
        "                                                            metric_name=metric), axis=1)\n",
        "    print(\"PEFT Model Summary Score for {0} : {1}\".format(metric, df[col_name_model].describe()))\n",
        "\n",
        "df.to_csv('falcon_7b_LoRA_r16_dialogue_summarization_12_13_2023_results_eval.csv')"
      ],
      "metadata": {
        "id": "nZM87Ov92wt7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}